<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content="d1vEtq-tAdSZMhm95XusIpEztC6ulceoa2UxqQhTEFY"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Rahul Anand Sharma</title> <meta name="author" content="Rahul Anand Sharma"/> <meta name="description" content="An up-to-date list is available on <a target='_blank' href='https://scholar.google.com/citations?user=gQBsCpsAAAAJ&hl=en'>Google Scholar</a>" /> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://rahul-anand.github.io//publications/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Rahul Anand Sharma</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/patents/">Patents</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">An up-to-date list is available on <a target="_blank" href="https://scholar.google.com/citations?user=gQBsCpsAAAAJ&amp;hl=en" rel="noopener noreferrer">Google Scholar</a></p> </header> <article> <div class="publications"> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/lumen.png"> <abbr class="badge"> <a href="https://conferences2.sigcomm.org/co-next/2022/#!/home" target="_blank" rel="noopener noreferrer">CoNEXT</a></abbr> </div> <div id="sharmalumen" class="col-sm-8"> <div class="title">Lumen: A Framework for Developing and Evaluating ML-Based IoT Network Anomaly Detection</div> <div class="author"> <em>Sharma, Rahul Anand</em>, Sabane, Ishan, <a href="https://netsyn.princeton.edu/" target="_blank" rel="noopener noreferrer">Apostolaki, Maria</a>, <a href="https://users.ece.cmu.edu/~agr/" target="_blank" rel="noopener noreferrer">Rowe, Anthony</a>,  and <a href="https://users.ece.cmu.edu/~vsekar/" target="_blank" rel="noopener noreferrer">Sekar, Vyas</a> </div> <div class="periodical"> <em>In CoNEXT</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/lumen22.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/rahul-anand/Lumen" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>The rise of IoT devices brings a lot of security risks. To mitigate them, researchers have introduced various promising network-based anomaly detection algorithms, which oftentimes leverage machine learning. Unfortunately, though, their deployment and further improvement by network operators and the research community are hampered. We believe this is due to three key reasons. First, known ML-based anomaly detection algorithms are evaluated –in the best case– on a couple of publicly available datasets, making it hard to compare across algorithms. Second, each ML-based IoT anomaly-detection algorithm makes assumptions about attacker practices/classification granularity, which reduce their applicability. Finally, the implementation of those algorithms is often monolithic, prohibiting code reuse. To ease deployment and promote research in this area, we present Lumen. Lumen is a modular framework paired with a benchmarking suite that allows users to efficiently develop, evaluate, and compare IoT ML-based anomaly detection algorithms. We demonstrate the utility of Lumen by implementing state-of-the-art anomaly detection algorithms and faithfully evaluating them on various datasets. Among other interesting insights that could inform real-world deployments and future research, using Lumen, we were able to identify what algorithms are most suitable to detect particular types of attacks. Lumen can also be used to construct new algorithms with better performance by combining the building blocks of competing efforts and improving the training setup.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/lumos.png"> <abbr class="badge"> <a href="https://www.usenix.org/conferences/byname/108" target="_blank" rel="noopener noreferrer">USENIX Security</a></abbr> </div> <div id="sharmalumos" class="col-sm-8"> <div class="title">Lumos: Identifying and Localizing Diverse Hidden IoT Devices in an Unfamiliar Environment</div> <div class="author"> <em>Sharma, Rahul Anand</em>, <a href="https://www.elahe.web.illinois.edu/" target="_blank" rel="noopener noreferrer">Soltanaghaei, Elahe</a>, <a href="https://users.ece.cmu.edu/~agr/" target="_blank" rel="noopener noreferrer">Rowe, Anthony</a>,  and <a href="https://users.ece.cmu.edu/~vsekar/" target="_blank" rel="noopener noreferrer">Sekar, Vyas</a> </div> <div class="periodical"> <em>In USENIX Security</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/lumos22.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/rahul-anand/Lumos" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a target="_blank" href="/assets/pdf/sec22_slides-sharma_rahul.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Hidden IoT devices are increasingly being used to snoop on users in hotel rooms or AirBnBs. We envision empowering users entering such unfamiliar environments to identify and locate (e.g., hidden camera behind plants) diverse hidden devices (e.g., cameras, microphones, speakers) using only their personal handhelds. What makes this challenging is the limited network visiibility and physical access that a user has in such unfamiliar environments, coupled with the lack of specialized equipment. This paper presents Lumos, a system that runs on commmodity user devices (e.g., phone, laptop) and enables users to identify and locate WiFi-connected hidden IoT devices and visualize their presence using an augmented reality interface. Lumos addresses key challenges in: (1) identifying diverse devices using only coarse-grained wireless layer features, without IP/DNS layer information and without knowledge of the WiFi channel assignments of the hidden devices; and (2) locating the identified IoT devices with respect to the user using only phone sensors and wireless signal strength measurements. We evaluated Lumos across 44 different IoT devices spanning various types, models, and brands across six different environments. Our results show that Lumos can identify hidden devices with 95% accuracy and locate them with a median error of 1.5m within 30 minutes in a two-bedroom, 1000 sq. ft. apartment</p> </div> </div> </div> </li> </ol> <h2 class="year">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/ampmap.png"> <abbr class="badge"> <a href="https://www.usenix.org/conferences/byname/108" target="_blank" rel="noopener noreferrer">USENIX Security</a></abbr> </div> <div id="moon2021accurately" class="col-sm-8"> <div class="title">Accurately Measuring Global Risk of Amplification Attacks using {AmpMap}</div> <div class="author"> <a href="https://soojinmoon.com/" target="_blank" rel="noopener noreferrer">Moon, Soo-Jin</a>, Yin, Yucheng,  <em>Sharma, Rahul Anand</em>, Yuan, Yifei, Spring, Jonathan M,  and <a href="https://users.ece.cmu.edu/~vsekar/" target="_blank" rel="noopener noreferrer">Sekar, Vyas</a> </div> <div class="periodical"> <em>In USENIX Security</em> 2021 </div> <div class="links"> <a target="_blank" href="/assets/pdf/ampmap21.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li></ol> <h2 class="year">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/slomo.png"> <abbr class="badge"> <a href="https://sigcomm.org/events/sigcomm-conference" target="_blank" rel="noopener noreferrer">SIGCOMM</a></abbr> </div> <div id="manousis2020contention" class="col-sm-8"> <div class="title">Contention-aware performance prediction for virtualized network functions</div> <div class="author"> <a href="https://www.andrew.cmu.edu/user/amanousi/" target="_blank" rel="noopener noreferrer">Manousis, Antonis</a>,  <em>Sharma, Rahul Anand</em>, <a href="https://users.ece.cmu.edu/~vsekar/" target="_blank" rel="noopener noreferrer">Sekar, Vyas</a>,  and Sherry, Justine </div> <div class="periodical"> <em>In SIGCOMM</em> 2020 </div> <div class="links"> <a target="_blank" href="/assets/pdf/slomo20.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/glitter1-PhotoRoom.png"> <abbr class="badge"> <a href="https://ipsn.acm.org/" target="_blank" rel="noopener noreferrer">IPSN</a></abbr> </div> <div id="sharma2020all" class="col-sm-8"> <div class="title">All that glitters: Low-power spoof-resilient optical markers for augmented reality</div> <div class="author"> <em>Sharma, Rahul Anand</em>, Dongare, Adwait, Miller, John, Wilkerson, Nicholas, Cohen, Daniel, <a href="https://users.ece.cmu.edu/~vsekar/" target="_blank" rel="noopener noreferrer">Sekar, Vyas</a>, Dutta, Prabal,  and <a href="https://users.ece.cmu.edu/~agr/" target="_blank" rel="noopener noreferrer">Rowe, Anthony</a> </div> <div class="periodical"> <em>In IPSN</em> 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/glitter20.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a target="_blank" href="/assets/pdf/ipsn_ppt.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>One of the major challenges faced by Augmented Reality (AR) systems is linking virtual content accurately on physical objects and locations. This problem is amplified for applications like mobile payment, device control or secure pairing that requires authentication. In this paper, we present an active LED tag system called GLITTER that uses a combination of Bluetooth Low-Energy (BLE) and modulated LEDs to anchor AR content with no a priori training or labeling of an environment. Unlike traditional optical markers that encode data spatially, each active optical marker encodes a tag’s identifier by blinking over time, improving both the tag density and range compared to AR tags and QR codes.We show that with a low-power BLE-enabled micro-controller and a single 5 mm LED, we are able to accurately link AR content from potentially hundreds of tags simultaneously on a standard mobile phone from as far as 30 meters. Expanding upon this, using active optical markers as a primitive, we show how a constellation of active optical markers can be used for full 3D pose estimation, which is required for many AR applications, using either a single LED on a planar surface or two or more arbitrarily positioned LEDs. Our design supports 108 unique codes in a single field of view with a detection latency of less than 400 ms even when held by hand.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"><abbr class="badge"><a href="http://buildsys.acm.org/" target="_blank" rel="noopener noreferrer">BuildSys</a></abbr></div> <div id="soltanaghaei2020robust" class="col-sm-8"> <div class="title">Robust and practical WiFi human sensing using on-device learning with a domain adaptive model</div> <div class="author"> <a href="https://www.elahe.web.illinois.edu/" target="_blank" rel="noopener noreferrer">Soltanaghaei, Elahe</a>,  <em>Sharma, Rahul Anand</em>, Wang, Zehao, Chittilappilly, Adarsh, Luong, Anh, Giler, Eric, Hall, Katie, Elias, Steve,  and <a href="https://users.ece.cmu.edu/~agr/" target="_blank" rel="noopener noreferrer">Rowe, Anthony</a> </div> <div class="periodical"> <em>In BuildSys</em> 2020 </div> <div class="links"> <a target="_blank" href="/assets/pdf/buildsys20.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="year">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 abbr"><abbr class="badge"><a href="https://compass.acm.org/" target="_blank" rel="noopener noreferrer">ACM COMPASS</a></abbr></div> <div id="swamy2019low" class="col-sm-8"> <div class="title">Low-cost aerial imaging for small holder farmers</div> <div class="author"> Swamy, AN, Kumar, Akshit, Patil, Rohit, Jain, Aditya, Kapetanovic, Zerina,  <em>Sharma, Rahul</em>, Vasisht, Deepak, Swaminathan, Manohar, <a href="https://www.microsoft.com/en-us/research/people/ranveer/" target="_blank" rel="noopener noreferrer">Chandra, Ranveer</a>, Badam, Anirudh,  and others, </div> <div class="periodical"> <em>In ACM COMPASS</em> 2019 </div> <div class="links"> <a target="_blank" href="/assets/pdf/compass19.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li></ol> <h2 class="year">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 abbr"><abbr class="badge"><a href="https://ieeexplore.ieee.org/xpl/conhome/1000040/all-proceedings" target="_blank" rel="noopener noreferrer">WACV</a></abbr></div> <div id="sharma2018automated" class="col-sm-8"> <div class="title">Automated top view registration of broadcast football videos</div> <div class="author"> <em>Sharma, Rahul Anand</em>, Bhat, Bharath, <a href="https://faculty.iiit.ac.in/~vgandhi/" target="_blank" rel="noopener noreferrer">Gandhi, Vineet</a>,  and <a href="https://faculty.iiit.ac.in/~jawahar/" target="_blank" rel="noopener noreferrer">Jawahar, CV</a> </div> <div class="periodical"> <em>In WACV</em> 2018 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/wacv18.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a target="_blank" href="/assets/pdf/WACV18-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a target="_blank" href="/assets/pdf/WACV_ppt.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>In this paper, we propose a novel method to register football broadcast video frames on the static top view model of the playing surface. The proposed method is fully automatic in contrast to the current state of the art which requires manual initialization of point correspondences between the image and the static model. Automatic registration using existing approaches has been difficult due to the lack of sufficient point correspondences. We investigate an alternate approach exploiting the edge information from the line markings on the field. We formulate the registration problem as a nearest neighbour search over a synthetically generated dictionary of edge map and homography pairs. The synthetic dictionary generation allows us to exhaustively cover a wide variety of camera angles and positions and reduce this problem to a minimal per-frame edge map matching procedure. We show that the per-frame results can be improved in videos using an optimization framework for temporal camera stabilization. We demonstrate the efficacy of our approach by presenting extensive results on a dataset collected from matches of football World Cup 2014. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"><abbr class="badge"><a href="https://sensys.acm.org/" target="_blank" rel="noopener noreferrer">SenSys</a></abbr></div> <div id="chakraborty2018fall" class="col-sm-8"> <div class="title">Fall-curve: A novel primitive for IoT Fault Detection and Isolation</div> <div class="author"> <a href="https://www.microsoft.com/en-us/research/people/tusherc/" target="_blank" rel="noopener noreferrer">Chakraborty, Tusher</a>, Nambi, Akshay Uttama, <a href="https://www.microsoft.com/en-us/research/people/ranveer/" target="_blank" rel="noopener noreferrer">Chandra, Ranveer</a>,  <em>Sharma, Rahul</em>, Swaminathan, Manohar, Kapetanovic, Zerina,  and Appavoo, Jonathan </div> <div class="periodical"> <em>In SenSys</em> 2018 </div> <div class="links"> <a target="_blank" href="/assets/pdf/sensys18.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"><abbr class="badge">Arxiv</abbr></div> <div id="sharma2018learnability" class="col-sm-8"> <div class="title">Learnability of learned neural networks</div> <div class="author"> <em>Sharma, Rahul Anand</em>, Goyal, Navin, Choudhury, Monojit,  and Netrapalli, Praneeth </div> <div class="periodical"> <em></em> 2018 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/learning18.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p> This paper explores the simplicity of learned neural networks under various settings: learned on real vs random data, varying size/architecture and using large minibatch size vs small minibatch size. The notion of simplicity used here is that of learnability i.e., how accurately can the prediction function of a neural network be learned from labeled samples from it. While learnability is different from (in fact often higher than) test accuracy, the results herein suggest that there is a strong correlation between small generalization errors and high learnability. This work also shows that there exist significant qualitative differences in shallow networks as compared to popular deep networks. More broadly, this paper extends in a new direction, previous work on understanding the properties of learned neural networks. Our hope is that such an empirical study of understanding learned neural networks might shed light on the right assumptions that can be made for a theoretical study of deep learning</p> </div> </div> </div> </li> </ol> <h2 class="year">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 abbr"><abbr class="badge"><a href="https://www.springer.com/journal/11760" target="_blank" rel="noopener noreferrer">SIVP</a></abbr></div> <div id="sharma2017automatic" class="col-sm-8"> <div class="title">Automatic analysis of broadcast football videos using contextual priors</div> <div class="author"> <em>Sharma, Rahul Anand</em>, <a href="https://faculty.iiit.ac.in/~vgandhi/" target="_blank" rel="noopener noreferrer">Gandhi, Vineet</a>, Chari, Visesh,  and <a href="https://faculty.iiit.ac.in/~jawahar/" target="_blank" rel="noopener noreferrer">Jawahar, CV</a> </div> <div class="periodical"> <em>Signal, Image and Video Processing</em> 2017 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/sivp18.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The presence of standard video editing practices in broadcast sports videos, like football, effectively means that such videos have stronger contextual priors than most generic videos. In this paper, we show that such information can be harnessed for automatic analysis of sports videos. Specifically, given an input video, we output per-frame information about camera angles and the events (goal, foul, etc.). Our main insight is that in the presence of temporal context (camera angles) for a video, the problem of event tagging (fouls, corners, goals, etc.) can be cast as per frame multiclass classification problem. We show that even with simple classifiers like linear SVM, we get significant improvement in the event tagging task when contextual information is included. We present extensive results for 10 matches from the recently concluded Football World Cup, to demonstrate the effectiveness of our approach. </p> </div> </div> </div> </li></ol> <h2 class="year">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 abbr"><abbr class="badge"><a href="https://dl.acm.org/conference/icvgip" target="_blank" rel="noopener noreferrer">ICVGIP</a></abbr></div> <div id="saraogi2016event" class="col-sm-8"> <div class="title">Event recognition in broadcast soccer videos</div> <div class="author"> Saraogi, Himangi,  <em>Sharma, Rahul Anand</em>,  and Kumar, Vijay </div> <div class="periodical"> <em>In ICVGIP</em> 2016 </div> <div class="links"> <a target="_blank" href="/assets/pdf/icvgip16.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li></ol> <h2 class="year">2015</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 abbr"><abbr class="badge"><a href="https://link.springer.com/conference/acpr" target="_blank" rel="noopener noreferrer">ACPR</a></abbr></div> <div id="sharma2015fine" class="col-sm-8"> <div class="title">Fine-grain annotation of cricket videos</div> <div class="author"> <em>Sharma, Rahul Anand</em>, Sankar, K Pramod,  and <a href="https://faculty.iiit.ac.in/~jawahar/" target="_blank" rel="noopener noreferrer">Jawahar, CV</a> </div> <div class="periodical"> <em>In ACPR</em> 2015 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/acpr15.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a target="_blank" href="/assets/pdf/acpr_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p> The recognition of human activities is one of the key problems in video understanding. Action recognition is challenging even for specific categories of videos, such as sports, that contain only a small set of actions. Interestingly, sports videos are accompanied by detailed commentaries available online, which could be used to perform action annotation in a weakly-supervised setting. For the specific case of Cricket videos, we address the challenge of temporal segmentation and annotation of actions with semantic descriptions. Our solution consists of two stages. In the first stage, the video is segmented into “scenes”, by utilizing the scene category information extracted from text-commentary. The second stage consists of classifying video-shots as well as the phrases in the textual description into various categories. The relevant phrases are then suitably mapped to the video-shots. The novel aspect of this work is the fine temporal scale at which semantic information is assigned to the video. As a result of our approach, we enable retrieval of specific actions that last only a few seconds, from several hours of video. This solution yields a large number of labelled exemplars, with no manual effort, that could be used by machine learning algorithms to learn complex actions. </p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Rahul Anand Sharma. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. Last updated: June 26, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-QFC7P0DSRN"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-QFC7P0DSRN");</script> </body> </html>