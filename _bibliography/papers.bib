---
---


@inproceedings{sharmalumos,
  abbr={USENIX Security},
    pdf={lumos22.pdf},

abstract={Hidden IoT devices are increasingly being used to snoop
on users in hotel rooms or AirBnBs. We envision empowering
users entering such unfamiliar environments to identify and
locate (e.g., hidden camera behind plants) diverse hidden
devices (e.g., cameras, microphones, speakers) using only
their personal handhelds.
What makes this challenging is the limited network visiibility and physical access that a user has in such unfamiliar
environments, coupled with the lack of specialized equipment.

This paper presents Lumos, a system that runs on commmodity user devices (e.g., phone, laptop) and enables users to
identify and locate WiFi-connected hidden IoT devices and
visualize their presence using an augmented reality interface.
Lumos addresses key challenges in: (1) identifying diverse
devices using only coarse-grained wireless layer features,
without IP/DNS layer information and without knowledge
of the WiFi channel assignments of the hidden devices; and
(2) locating the identified IoT devices with respect to the
user using only phone sensors and wireless signal strength
measurements. We evaluated Lumos across 44 different IoT
devices spanning various types, models, and brands across
six different environments. Our results show that Lumos
can identify hidden devices with 95% accuracy and locate
them with a median error of 1.5m within 30 minutes in a
two-bedroom, 1000 sq. ft. apartment},
title = {Lumos: Identifying and Localizing Diverse Hidden {IoT} Devices in an Unfamiliar Environment},
  author={Sharma, Rahul Anand and Soltanaghaei, Elahe and Rowe, Anthony and Sekar, Vyas},
    booktitle={USENIX Security},

    year={2022},
      selected={true},
        preview={lumos.png},
        slides={sec22_slides-sharma_rahul.pdf},
        code={https://github.com/rahul-anand/Lumos},



}


@inproceedings{sharma2020all,
abbr={IPSN},
    pdf={glitter20.pdf},
    slides={ipsn_ppt.pdf},

abstract={One of the major challenges faced by Augmented Reality (AR) systems is linking virtual content accurately on physical objects and locations. This problem is amplified for applications like mobile payment, device control or secure pairing that requires authentication. In this paper, we present an active LED tag system called GLITTER that uses a combination of Bluetooth Low-Energy (BLE) and modulated LEDs to anchor AR content with no a priori training or labeling of an environment. Unlike traditional optical markers that encode data spatially, each active optical marker encodes a tag’s identifier by blinking over time, improving both the tag density and range compared to AR tags and QR codes.We show that with a low-power BLE-enabled micro-controller and a single 5 mm LED, we are able to accurately link AR content from potentially hundreds of tags simultaneously on a standard mobile phone from as far as 30 meters. Expanding upon this, using active optical markers as a primitive, we show how a constellation of active optical markers can be used for full 3D pose estimation, which is required for many AR applications, using either a single LED on a planar surface or two or more arbitrarily positioned LEDs. Our design supports 108 unique codes in a single field of view with a detection latency of less than 400 ms even when held by hand.},
  title={All that glitters: Low-power spoof-resilient optical markers for augmented reality},
  author={Sharma, Rahul Anand and Dongare, Adwait and Miller, John and Wilkerson, Nicholas and Cohen, Daniel and Sekar, Vyas and Dutta, Prabal and Rowe, Anthony},
  booktitle={IPSN},
  pages={289--300},
  year={2020},
        selected={true},
                preview={glitter1-PhotoRoom.png},


  organization={IEEE}
}


@inproceedings{sharma2018automated,
  abbr={WACV},
    pdf={wacv18.pdf},
abstract={In this paper, we propose a novel method to register football broadcast video frames on the static top view model
of the playing surface. 
The proposed method is fully automatic in contrast to the current state of the art which requires manual initialization of point correspondences between the image and the static model.
 Automatic registration using existing approaches has been difficult due to the
lack of sufficient point correspondences. We investigate an
alternate approach exploiting the edge information from the
line markings on the field. We formulate the registration
problem as a nearest neighbour search over a synthetically
generated dictionary of edge map and homography pairs.
The synthetic dictionary generation allows us to exhaustively cover a wide variety of camera angles and positions
and reduce this problem to a minimal per-frame edge map
matching procedure. We show that the per-frame results can
be improved in videos using an optimization framework for
temporal camera stabilization. We demonstrate the efficacy
of our approach by presenting extensive results on a dataset
collected from matches of football World Cup 2014.
},

  title={Automated top view registration of broadcast football videos},
  author={Sharma, Rahul Anand and Bhat, Bharath and Gandhi, Vineet and Jawahar, CV},
  booktitle={WACV},
  pages={305--313},
  year={2018},
  organization={IEEE},
        selected={true},
                slides={WACV_ppt.pdf},
                poster={WACV18-poster.pdf},


}
@inproceedings{chakraborty2018fall,
  abbr={SenSys},
    pdf={sensys18.pdf},

  title={Fall-curve: A novel primitive for IoT Fault Detection and Isolation},
  author={Chakraborty, Tusher and Nambi, Akshay Uttama and Chandra, Ranveer and Sharma, Rahul and Swaminathan, Manohar and Kapetanovic, Zerina and Appavoo, Jonathan},
  booktitle={SenSys},
  pages={95--107},
  year={2018}
}
@inproceedings{manousis2020contention,
  abbr={SIGCOMM},
    pdf={slomo20.pdf},

  title={Contention-aware performance prediction for virtualized network functions},
  author={Manousis, Antonis and Sharma, Rahul Anand and Sekar, Vyas and Sherry, Justine},
  booktitle={SIGCOMM},
  pages={270--282},
  
  year={2020}
}

@inproceedings{swamy2019low,
abbr={ACM COMPASS},
    pdf={compass19.pdf},

  title={Low-cost aerial imaging for small holder farmers},
  author={Swamy, AN and Kumar, Akshit and Patil, Rohit and Jain, Aditya and Kapetanovic, Zerina and Sharma, Rahul and Vasisht, Deepak and Swaminathan, Manohar and Chandra, Ranveer and Badam, Anirudh and others},
  year={2019},
    booktitle={ACM COMPASS},

}
@article{sharma2017automatic,
  abbr={SIVP},
  abstract={The presence of standard video editing practices
in broadcast sports videos, like football, effectively means
that such videos have stronger contextual priors than most
generic videos. In this paper, we show that such information can be harnessed for automatic analysis of sports videos.
Specifically, given an input video, we output per-frame information about camera angles and the events (goal, foul, etc.).
Our main insight is that in the presence of temporal context
(camera angles) for a video, the problem of event tagging
(fouls, corners, goals, etc.) can be cast as per frame multiclass classification problem. We show that even with simple
classifiers like linear SVM, we get significant improvement
in the event tagging task when contextual information is
included. We present extensive results for 10 matches from
the recently concluded Football World Cup, to demonstrate
the effectiveness of our approach.
  },
    pdf={sivp18.pdf},

  title={Automatic analysis of broadcast football videos using contextual priors},
  author={Sharma, Rahul Anand and Gandhi, Vineet and Chari, Visesh and Jawahar, CV},
  journal={Signal, Image and Video Processing},
  volume={11},
  number={1},
        selected={true},

  pages={171--178},
  year={2017},
  publisher={Springer}
}
@inproceedings{moon2021accurately,
  abbr={USENIX Security},
    pdf={ampmap21.pdf},

  title={Accurately Measuring Global Risk of Amplification Attacks using $\{$AmpMap$\}$},
  author={Moon, Soo-Jin and Yin, Yucheng and Sharma, Rahul Anand and Yuan, Yifei and Spring, Jonathan M and Sekar, Vyas},
  booktitle={USENIX Security},
  pages={3881--3898},
  year={2021}
}

@inproceedings{soltanaghaei2020robust,
  abbr={BuidSys},
  pdf={buildsys20.pdf},

  title={Robust and practical WiFi human sensing using on-device learning with a domain adaptive model},
  author={Soltanaghaei, Elahe and Sharma, Rahul Anand and Wang, Zehao and Chittilappilly, Adarsh and Luong, Anh and Giler, Eric and Hall, Katie and Elias, Steve and Rowe, Anthony},
  booktitle={BuildSys},
  pages={150--159},
  year={2020}
}
@inproceedings{sharma2015fine,
  abbr={ACPR},
  pdf={acpr15.pdf},
abstract={
The recognition of human activities is one of the key
problems in video understanding. Action recognition is
challenging even for specific categories of videos, such as
sports, that contain only a small set of actions. Interestingly, sports videos are accompanied by detailed commentaries available online, which could be used to perform action annotation in a weakly-supervised setting.
 For the specific case of Cricket videos, we address the challenge of
temporal segmentation and annotation of actions with semantic descriptions. Our solution consists of two stages.
In the first stage, the video is segmented into “scenes”, by
utilizing the scene category information extracted from text-commentary. The second stage consists of classifying video-shots as well as the phrases in the textual description into
various categories. The relevant phrases are then suitably
mapped to the video-shots. The novel aspect of this work
is the fine temporal scale at which semantic information is
assigned to the video. As a result of our approach, we enable retrieval of specific actions that last only a few seconds, from several hours of video. This solution yields a
large number of labelled exemplars, with no manual effort,
that could be used by machine learning algorithms to learn
complex actions.
},
  poster={acpr_poster.pdf},

  title={Fine-grain annotation of cricket videos},
  author={Sharma, Rahul Anand and Sankar, K Pramod and Jawahar, CV},
        selected={true},

  booktitle={ACPR},
  pages={421--425},
  year={2015},
  organization={IEEE}
}
@inproceedings{saraogi2016event,
  abbr={ICVGIP},
  pdf={icvgip16.pdf},

  title={Event recognition in broadcast soccer videos},
  author={Saraogi, Himangi and Sharma, Rahul Anand and Kumar, Vijay},
  booktitle={ICVGIP},
  pages={1--7},
  year={2016}
}
@article{sharma2018learnability,
abbr={Arxiv},
abstract={
This paper explores the simplicity of learned neural networks under various settings: learned on real vs random data, varying size/architecture and using large
minibatch size vs small minibatch size. The notion of simplicity used here is that
of learnability i.e., how accurately can the prediction function of a neural network be learned from labeled samples from it. While learnability is different from
(in fact often higher than) test accuracy, the results herein suggest that there is a
strong correlation between small generalization errors and high learnability. This
work also shows that there exist significant qualitative differences in shallow networks as compared to popular deep networks. More broadly, this paper extends
in a new direction, previous work on understanding the properties of learned neural networks. Our hope is that such an empirical study of understanding learned
neural networks might shed light on the right assumptions that can be made for a
theoretical study of deep learning},

  pdf={learning18.pdf},

  title={Learnability of learned neural networks},
        selected={true},

  author={Sharma, Rahul Anand and Goyal, Navin and Choudhury, Monojit and Netrapalli, Praneeth},
  year={2018}
}

